{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import numpy as np\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A classic fill-in-the-blank!\n",
      "\n",
      "Bill Gates worked at Microsoft.\n",
      "\n",
      "So, I'll change the tag [MASK] to: **Microsoft**\n"
     ]
    }
   ],
   "source": [
    "# LangChain supports many other chat models. Here, we're using Ollama\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# supports many more optional parameters. Hover on your `ChatOllama(...)`\n",
    "# class to view the latest available supported parameters\n",
    "llm = ChatOllama(model=\"llama3\", seed=42)\n",
    "prompt = ChatPromptTemplate.from_template(\"Bill Gates worked at {topic}. Change the tag [MASK] to the correct token.\")\n",
    "\n",
    "# using LangChain Expressive Language chain syntax\n",
    "# learn more about the LCEL on\n",
    "# /docs/concepts/#langchain-expression-language-lcel\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# for brevity, response is printed in terminal\n",
    "# You can use LangServe to deploy your application for\n",
    "# production\n",
    "print(chain.invoke({\"topic\": \"[MASK]\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why\n",
      " did\n",
      " the\n",
      " astronaut\n",
      " break\n",
      " up\n",
      " with\n",
      " his\n",
      " girlfriend\n",
      " before\n",
      " going\n",
      " to\n",
      " Mars\n",
      "?\n",
      "\n",
      "\n",
      "Because\n",
      " he\n",
      " needed\n",
      " space\n",
      "!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topic = {\"topic\": \"Space travel\"}\n",
    "\n",
    "async for chunks in chain.astream(topic):\n",
    "    print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I apologize, but I don't see any information in the provided JSON schema about Bill Gates. The schema appears to be empty. If you meant to provide a different piece of data or a specific person's details, please feel free to share it, and I'll do my best to help!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "json_schema = {\n",
    "    \"title\": \"Companie\",\n",
    "    \"description\": \"Identifie the name and change the tag [MASK] by the correct answer.\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"name\": {\"title\": \"Name\", \"description\": \"The person's name\", \"type\": \"string\"},\n",
    "        \"mask\": {\"title\": \"Mask\", \"description\": \"the word which took place of the tag [MASK]\", \"type\": \"string\"},\n",
    "        \"fav_food\": {\n",
    "            \"title\": \"CEO of the company\",\n",
    "            \"description\": \"where the person works\",\n",
    "            \"type\": \"string\",\n",
    "        },\n",
    "    },\n",
    "    \"required\": [\"name\", \"mask\"],\n",
    "}\n",
    "\n",
    "llm = ChatOllama(model=\"llama3\")\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(\n",
    "        content=\"Please tell me about a person using the following JSON schema:\"\n",
    "    ),\n",
    "    HumanMessage(content=\"{dumps}\"),\n",
    "    HumanMessage(\n",
    "        content=\"Now, considering the schema, tell me where Bill Gates works for?\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "dumps = json.dumps(json_schema, indent=2)\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "print(chain.invoke({\"dumps\": dumps}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelfile='''\n",
    "FROM llama3\n",
    "SYSTEM You only answer with json schema.\n",
    "'''\n",
    "\n",
    "ollama.create(model='json_llama3', modelfile=modelfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelfile='''\n",
    "FROM llama3\n",
    "SYSTEM You only predict the correct token for the [MASK] tag and provide the 10 most suitable words from highest to lowest probability.\n",
    "'''\n",
    "\n",
    "ollama.create(model='mask_llama3-top10', modelfile=modelfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelfile='''\n",
    "FROM llama3\n",
    "SYSTEM You are mario from super mario bros.\n",
    "'''\n",
    "\n",
    "ollama.create(model='example', modelfile=modelfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(model='mask_llama3', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Steve Jobs works for [MASK]',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success'}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelfile='''\n",
    "FROM gemma\n",
    "SYSTEM You only predict the correct token for the [MASK] tag and provide the 10 most suitable words, from the highest to the lowest probability, only with token and probability. The answer should be in dictionary format with the following form list[{'token': token, 'prob': probability}, {'token': token, 'prob': probability},{'token': token, 'prob': probability}, {'token': token, 'prob': probability},{'token': token, 'prob': probability}, {'token': token, 'prob': probability},{'token': token, 'prob': probability}, {'token': token, 'prob': probability},{'token': token, 'prob': probability}, {'token': token, 'prob': probability}]\n",
    "'''\n",
    "\n",
    "ollama.create(model='mask_gemma-top10_v1', modelfile=modelfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the top 10 predictions for the [MASK] token, along with their corresponding probabilities:\n",
      "\n",
      "1. Apple (0.95)\n",
      "2. NeXT (0.03)\n",
      "3. Pixar (0.01)\n",
      "4. Microsoft (0.005)\n",
      "5. Google (0.003)\n",
      "6. HP (0.002)\n",
      "7. IBM (0.001)\n",
      "8. Oracle (0.0005)\n",
      "9. Intel (0.0002)\n",
      "10. Dell (0.0001)\n",
      "\n",
      "Note that the probabilities are approximate and based on my training data.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = ollama.chat(model='mask_llama3-top10', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Steve Jobs works for [MASK]',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the top 10 predictions for the [MASK] token:\n",
      "\n",
      "1. Apple, prob: 0.95\n",
      "2. Pixar, prob: 0.04\n",
      "3. NeXT, prob: 0.01\n",
      "\n",
      "Note that Steve Jobs co-founded Apple and later returned to the company after a hiatus during which he founded NeXT and acquired Pixar (which was then an animation studio).\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(model='mask_llama3-top10_v2', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Steve Jobs works for [MASK]',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the top 10 predictions for the [MASK] token:\n",
      "\n",
      "token | prob\n",
      "-----|-----\n",
      "Apple | 0.92\n",
      "NeXT | 0.05\n",
      "Pixar | 0.02\n",
      "Google | 0.01\n",
      "Microsoft | 0.01\n",
      "Amazon | 0.001\n",
      "Facebook | 0.001\n",
      "Twitter | 0.0005\n",
      "Yahoo | 0.0003\n",
      "Sony | 0.0001\n",
      "\n",
      "The most likely answer is that Steve Jobs works for Apple, which is correct!\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(model='mask_llama3-top10_v3', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Steve Jobs works for [MASK]',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the top 10 predictions for the [MASK] tag:\n",
      "\n",
      "1. APPLE (0.935)\n",
      "2. Apple (0.024)\n",
      "3. Microsoft (0.003)\n",
      "4. Google (0.002)\n",
      "5. IBM (0.001)\n",
      "6. HP (0.001)\n",
      "7. Dell (0.000)\n",
      "8. Oracle (0.000)\n",
      "9. Intel (0.000)\n",
      "10. AMD (0.000)\n",
      "\n",
      "Note that the probabilities are approximate and may vary depending on the specific model used.\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(model='mask_llama3-top10_v4', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Steve Jobs works for [MASK]',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the top 10 predicted tokens along with their probabilities:\n",
      "\n",
      "{'Apple', '0.9999'}\n",
      "{'Microsoft', '0.0001'}\n",
      "{'Google', '0.00005'}\n",
      "{'IBM', '0.00004'}\n",
      "{'Oracle', '0.00003'}\n",
      "{'HP', '0.00002'}\n",
      "{'Intel', '0.00001'}\n",
      "{'Cisco', '0.000005'}\n",
      "{'SAP', '0.000004'}\n",
      "{'Dell', '0.000003'}\n",
      "\n",
      "Note: The probabilities are approximate and may vary depending on the model and training data used.\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(model='mask_llama3-top10_v5', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Steve Jobs works for [MASK]',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'token': 'Apple', 'prob': 0.934}, {'token': 'NeXT', 'prob': 0.0334}, {'token': 'Microsoft', 'prob': 0.0119}, {'token': 'Google', 'prob': 0.0083}, {'token': 'IBM', 'prob': 0.0056}, {'token': 'Oracle', 'prob': 0.0038}, {'token': 'Amazon', 'prob': 0.0024}, {'token': 'Facebook', 'prob': 0.0019}, {'token': 'HP', 'prob': 0.0011}, {'token': 'Intel', 'prob': 0.0007}]\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(model='mask_llama3-top10_v6', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Steve Jobs works for [MASK]',\n",
    "  },\n",
    "])\n",
    "answer = response['message']['content']\n",
    "print(answer)\n",
    "print(type(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'token': 'Apple', 'prob': 0.998}, {'token': 'Google', 'prob': 0.001}, {'token': 'Microsoft', 'prob': 0.0009}, {'token': 'Amazon', 'prob': 0.0008}, {'token': 'Facebook', 'prob': 0.0007}, {'token': 'Twitter', 'prob': 0.0006}, {'token': 'Samsung', 'prob': 0.0005}, {'token': 'Sony', 'prob': 0.0004}, {'token': 'Huawei', 'prob': 0.0003}, {'token': 'Intel', 'prob': 0.0002}]\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(model='mask_gemma-top10_v1', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Steve Jobs works for [MASK]',\n",
    "  },\n",
    "])\n",
    "answer = response['message']['content']\n",
    "print(answer)\n",
    "print(type(answer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
